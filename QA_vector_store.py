# QA_vector_store.py

import faiss
import numpy as np
import json
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import os

# === Chargement des variables d'environnement ===
load_dotenv()
hf_token = os.getenv("huggingface_token")

# === Chemins vers les fichiers FAISS ===
INDEX_PATH = "faiss_index.bin"
METADATA_PATH = "faiss_metadata.json"
EMBEDDING_MODEL = "nomic-ai/nomic-embed-text-v1"
EMBEDDING_DIM = 768
TOP_K = 5  # Nombre de passages les plus pertinents √† retourner

def check_faiss_index(index, metadata):
    """
    V√©rifie si l'index FAISS contient des √©l√©ments et si les m√©tadonn√©es sont valides.
    """
    # V√©rification de la taille de l'index
    print(f"üîç Taille de l'index FAISS : {index.ntotal}")
    print(f"üîç Taille de la metadata : {len(metadata)}")
    
    if index.ntotal == 0:
        print("‚ö†Ô∏è L'index FAISS est vide. Veuillez v√©rifier la cr√©ation de l'index.")
    if len(metadata) == 0:
        print("‚ö†Ô∏è Aucune donn√©e de m√©tadonn√©es trouv√©e. V√©rifiez la g√©n√©ration des embeddings.")

def test_embedding_generation(model):
    """
    Teste la g√©n√©ration des embeddings avec un exemple simple.
    """
    sample_query = "Test embedding"
    embedding = model.encode(sample_query)
    print(f"üîÑ Embedding pour '{sample_query}': {embedding[:10]}...")  # Afficher les 10 premiers √©l√©ments de l'embedding
    if not embedding.any():
        print("‚ö†Ô∏è Probl√®me avec la g√©n√©ration des embeddings.")
    else:
        print("‚úÖ Embedding g√©n√©r√© avec succ√®s.")




# === Chargement du mod√®le d'embedding ===
print("üîÑ Chargement du mod√®le d'embedding...")
model = SentenceTransformer(EMBEDDING_MODEL, trust_remote_code=True, use_auth_token=hf_token)


# Test de la g√©n√©ration des embeddings
test_embedding_generation(model)

# === Chargement de l'index FAISS ===
print("üì• Chargement de l‚Äôindex FAISS...")
index = faiss.read_index(INDEX_PATH)

# === Chargement des m√©tadonn√©es associ√©es ===
with open(METADATA_PATH, "r", encoding="utf-8") as f:
    metadata = json.load(f)

print(f"‚úÖ Index et m√©tadonn√©es charg√©s ({len(metadata)} chunks).")

# Test de l'index FAISS
check_faiss_index(index, metadata)

def search_similar_chunks(query, k=5):
    """
    Recherche les k chunks les plus similaires dans FAISS.
    """
    query_embedding = model.encode(query)
    query_embedding = np.array([query_embedding])
    
    # Effectuer la recherche dans FAISS
    _, indices = index.search(query_embedding, k)
    
    # D√©boguer les indices retourn√©s
    print(f"Indices retourn√©s par FAISS : {indices}")
    print(f"Taille de la metadata : {len(metadata)}")
    
    similar_chunks = []
    for idx in indices[0]:
        # Ajouter un contr√¥le pour s'assurer que l'indice est valide
        if idx >= 0 and idx < len(metadata):
            similar_chunks.append(metadata[idx])
        else:
            print(f"‚ö†Ô∏è L'index {idx} est hors de port√©e de la metadata. Ignor√©.")
    
    return similar_chunks


def build_context_from_query(query: str, k: int = TOP_K) -> str:
    """
    Construit un contexte concat√©n√© √† partir des meilleurs chunks
    """
    chunks = search_similar_chunks(query, k)
    context = "\n---\n".join([c["text"] for c in chunks])
    return context

