# generate_embeddings.py

import json
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import os
from dotenv import load_dotenv

# === Mode debug (affiche les embeddings FAISS) ===
DEBUG = False

# === Chargement des variables d'environnement ===
load_dotenv()
hf_token = os.getenv("huggingface_token")
if hf_token is None:
    print("‚ùå Hugging Face token not found in .env file.")
    exit(1)
print("‚úÖ Hugging Face token loaded.")

# === Configuration ===
MODEL_NAME = "nomic-ai/nomic-embed-text-v1"
OUTPUT_DIR = Path("output")
INDEX_PATH = "faiss_index.bin"
METADATA_PATH = "faiss_metadata.json"
EMBEDDING_DIM = 768

# === Chargement du mod√®le ===
print("üîÑ Chargement du mod√®le d'embedding...")
model = SentenceTransformer(MODEL_NAME, trust_remote_code=True, use_auth_token=hf_token)
print(f"‚úÖ Mod√®le charg√© : {MODEL_NAME}")

# === Chargement ou cr√©ation de l'index FAISS ===
if Path(INDEX_PATH).exists() and Path(METADATA_PATH).exists():
    print("üì• Chargement de l‚Äôindex FAISS existant...")
    index = faiss.read_index(INDEX_PATH)
    with open(METADATA_PATH, "r", encoding="utf-8") as f:
        metadata = json.load(f)
    print(f"‚úÖ Index existant charg√© avec {index.ntotal} vecteurs.")
else:
    print("‚öôÔ∏è Aucun index existant. Cr√©ation d‚Äôun nouvel index FAISS...")
    index = faiss.IndexHNSWFlat(EMBEDDING_DIM, 32)
    metadata = []

def generate_embeddings_for_new_files():
    """
    G√©n√®re des embeddings pour tous les fichiers 'cleaned_chunks.jsonl' non trait√©s.
    Ajoute les embeddings √† l'index FAISS et met √† jour les m√©tadonn√©es.
    """
    doc_folders = list(OUTPUT_DIR.iterdir())
    print(f"\nüìÅ {len(doc_folders)} documents √† traiter...\n")

    for doc_folder in tqdm(doc_folders, desc="üìö Traitement des documents"):
        chunk_path = doc_folder / "cleaned_chunks.jsonl"

        if not chunk_path.exists():
            print(f"‚ö†Ô∏è Aucun 'cleaned_chunks.jsonl' pour {doc_folder.name}, ignor√©.")
            continue

        # V√©rifie si ce document est d√©j√† dans les m√©tadonn√©es
        if any(m["doc_name"] == doc_folder.name for m in metadata):
            print(f"‚ö†Ô∏è {doc_folder.name} d√©j√† pr√©sent dans l'index. Ignor√©.")
            continue

        print(f"üîç Traitement de {doc_folder.name}...")

        with open(chunk_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        for line in tqdm(lines, desc=f"üß† Embedding {doc_folder.name}", leave=False):
            data = json.loads(line)
            chunk_text = data["text"]
            chunk_id = data["chunk_id"]

            # G√©n√©rer l'embedding
            try:
                embedding = model.encode(chunk_text)
                embedding_np = np.array([embedding])
                faiss.normalize_L2(embedding_np)
                index.add(embedding_np)
            except Exception as e:
                print(f"‚ùå Erreur lors de la g√©n√©ration de l'embedding pour {chunk_id}: {e}")
                continue

            # Mettre √† jour les m√©tadonn√©es
            metadata.append({
                "doc_name": doc_folder.name,
                "full_doc_path": str(doc_folder.resolve()),
                "chunk_id": chunk_id,
                "text": chunk_text[:100]  # Affiche les 100 premiers caract√®res du texte pour le log
            })

    # Sauvegarde
    print(f"\nüíæ Sauvegarde des r√©sultats...")
    faiss.write_index(index, INDEX_PATH)
    print(f"‚úÖ Index FAISS sauvegard√© ‚Üí {INDEX_PATH}")

    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ M√©tadonn√©es sauvegard√©es ‚Üí {METADATA_PATH}")

    print("\nüéâ Embeddings g√©n√©r√©s avec succ√®s !")

    # Debug : Afficher les 3 premiers vecteurs de FAISS
    if DEBUG:
        print("\nüß™ DEBUG : premiers vecteurs FAISS :")
        vectors = index.reconstruct_n(0, min(3, index.ntotal))
        for i, vec in enumerate(vectors):
            print(f"‚û°Ô∏è Vector {i} : {vec[:10]}...")  # Affiche les 10 premi√®res valeurs

print(index.ntotal)

# === Ex√©cution ===
if __name__ == "__main__":
    generate_embeddings_for_new_files()
